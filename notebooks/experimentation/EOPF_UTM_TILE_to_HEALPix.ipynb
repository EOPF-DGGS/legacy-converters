{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# EOPF: Sentinel‑2 UTM → HEALPix (Level‑1C)\n",
    "\n",
    "**Audience:** Researchers, Data Scientists\n",
    "**Last updated:** 2025-09-09\n",
    "\n",
    "This notebook demonstrates a clear, reproducible workflow to convert Sentinel‑2 L1C data\n",
    "from its native UTM projection to a global **HEALPix** grid, following the style of the\n",
    "[EOPF Sentinel‑2 examples].\n",
    "\n",
    "**You will learn to:**\n",
    "- Access cloud‑native Sentinel‑2 Zarr data (via STAC/EOPF)\n",
    "- Subset a region of interest (ROI)\n",
    "- Attach latitude/longitude coordinates to UTM gridded data\n",
    "- Reproject/aggregate onto a **HEALPix** equal‑area grid\n",
    "- Inspect and save the result for downstream analysis\n",
    "\n",
    "**Prerequisites:** Python, Xarray, familiarity with EO data, basic projections.\n",
    "\n",
    "> Tip: Run the notebook top‑to‑bottom in a fresh environment for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment & Dependencies](#environment--dependencies)\n",
    "3. [Data Access via STAC](#data-access-via-stac)\n",
    "4. [Open the Sentinel‑2 Product](#open-the-sentinel2-product)\n",
    "5. [Subset a Region of Interest](#subset-a-region-of-interest)\n",
    "6. [Quicklook & Visualization](#quicklook--visualization)\n",
    "7. [Add Latitude/Longitude Coordinates](#add-latitudelongitude-coordinates)\n",
    "8. [Convert to HEALPix](#convert-to-healpix)\n",
    "9. [Inspect the HEALPix Output](#inspect-the-healpix-output)\n",
    "10. [Save & Export](#save--export)\n",
    "11. [Appendix / References](#appendix--references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Sentinel‑2 Level‑1C products are distributed in **UTM** tiles (projected meters). Many global\n",
    "analyses benefit from a sphere‑aware, **equal‑area** grid. **HEALPix** (Hierarchical Equal Area\n",
    "isoLatitude Pixelization) partitions the globe into equal‑area cells, making aggregations and\n",
    "comparisons consistent across latitudes. This notebook restructures the original workflow with\n",
    "clear chapters and explanations while preserving all original code and outputs.\n",
    "\n",
    "This is a **Skelton** Notebook Converting EOPF Zarr format in UTM to HEALPix;\n",
    "We use EOPF Sample service data here.\n",
    "Since EOPF data is based on Datatree, we use that property. Thus this workflow can be applied to any UTM expressed EOPF ZARR format. (cf S2L1 and S2L2A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Setting up enviroment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4",
   "metadata": {},
   "source": [
    "try:\n",
    "    import healpix_geo  # noqa: F401\n",
    "except ImportError:\n",
    "    !pip install xarray-eopf  xdggs healpix-geo foscat==2025.10.2 pystac_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Imports and setup\n",
    "\n",
    "- Loads libraries: matplotlib, numpy, xarray. Configures plotting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyproj\n",
    "import pystac_client\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data Access via STAC\n",
    "\n",
    "We query the EOPF STAC catalog for a suitable Sentinel‑2 L1C item (e.g., low cloud cover, good sun elevation) and obtain the Zarr asset. This keeps the workflow cloud‑native and lazily loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Open datasets with Xarray\n",
    "\n",
    "\n",
    "- Opens datasets using Xarray (lazy loading with Dask if chunked).\n",
    "- To run this notebook with \"sentinel-2-l1c\", or \"sentinel-2-l2a\", simply specify it on the collections\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access cloud-optimized Sentinel-2 data via the EOPF STAC catalog\n",
    "catalog = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
    "\n",
    "# Define oceanographic study area and time window\n",
    "LON, LAT = -4.5, 48  # Bay of Biscay - known for consistent wave patterns\n",
    "date = \"2025-06-17/2025-06-17\"\n",
    "\n",
    "# Search criteria optimized for wave analysis\n",
    "\n",
    "collection = \"sentinel-2-l1c\"\n",
    "items = list(\n",
    "    catalog.search(\n",
    "        datetime=date,\n",
    "        collections=[collection],\n",
    "        intersects=dict(type=\"Point\", coordinates=[LON, LAT]),\n",
    "        query={\n",
    "            \"eo:cloud_cover\": {\n",
    "                \"lt\": 20\n",
    "            },  # Cloud cover < 20% ensures clear ocean surface\n",
    "            \"view:sun_elevation\": {\n",
    "                \"gt\": 25\n",
    "            },  # Filter for high sun elevation > 25° (→ sun zenith angle < 65°),\n",
    "            # which places the sun near the zenith.\n",
    "        },\n",
    "    ).items()\n",
    ")\n",
    "\n",
    "for item in items:\n",
    "    print(f\"✅ {item.id}\")\n",
    "\n",
    "item = items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Open the Sentinel‑2 Product\n",
    "\n",
    "We open the product directly as an **Xarray DataTree** (lazy) to navigate measurements, conditions, and multi‑resolution groups (10m/20m/60m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset lazily from object storage\n",
    "dt = xr.open_datatree(\n",
    "    item.assets[\"product\"].href,\n",
    "    **item.assets[\"product\"].extra_fields[\"xarray:open_datatree_kwargs\"],\n",
    ")\n",
    "\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Subset a Region of Interest\n",
    "\n",
    "We extract a small ROI (e.g., a 3×3 window) to demonstrate transformation and HEALPix conversion on a compact example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose small area in UTM\n",
    "## TODO, we can update here to chose  'one detection' area.\n",
    "\n",
    "small_dt = dt.sel(\n",
    "    x=slice(\n",
    "        dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x[0],\n",
    "        dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x[1],\n",
    "    ),\n",
    "    y=slice(\n",
    "        dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y[0],\n",
    "        dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y[1],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[\"conditions\"][\"geometry\"][\"sun_angles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Quicklook & Visualization\n",
    "\n",
    "We visualize one or more bands (e.g., B02) to confirm the ROI and interpret pixel values before reprojection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see , probably a boat.\n",
    "# Todo, add plot for small_dt['conditions']['l1c_quicklook']['r10m']\n",
    "# small_dt['conditions']['l1c_quicklook']['r10m'].hvplot.rgb(x='x',y='y', )\n",
    "\n",
    "small_dt[\"measurements\"][\"reflectance\"][\"r60m\"][\"b09\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Add Latitude/Longitude Coordinates\n",
    "\n",
    "Using **pyproj** we transform UTM (x/y in meters) to geographic coordinates (lon/lat in degrees) and attach them as auxiliary coordinates. This enables subsequent binning onto a spherical grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Annotate UTM with latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_latlon(ds: xr.Dataset, transformer: pyproj.Transformer) -> xr.Dataset:\n",
    "    \"\"\"Attach latitude/longitude coords + CF metadata to a Dataset with (x,y).\"\"\"\n",
    "    if not {\"x\", \"y\"}.issubset(ds.dims):\n",
    "        return ds\n",
    "\n",
    "    xx, yy = np.meshgrid(ds[\"x\"].values, ds[\"y\"].values, indexing=\"xy\")\n",
    "    lon, lat = transformer.transform(xx, yy)\n",
    "\n",
    "    ds = ds.assign_coords(\n",
    "        longitude=((\"y\", \"x\"), lon),\n",
    "        latitude=((\"y\", \"x\"), lat),\n",
    "    )\n",
    "    ds[\"latitude\"].attrs.update(\n",
    "        {\n",
    "            \"standard_name\": \"latitude\",\n",
    "            \"long_name\": \"Latitude\",\n",
    "            \"units\": \"degrees_north\",\n",
    "            \"axis\": \"Y\",\n",
    "        }\n",
    "    )\n",
    "    ds[\"longitude\"].attrs.update(\n",
    "        {\n",
    "            \"standard_name\": \"longitude\",\n",
    "            \"long_name\": \"Longitude\",\n",
    "            \"units\": \"degrees_east\",\n",
    "            \"axis\": \"X\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Make sure vars with (y,x) advertise the aux coords\n",
    "    for var in ds.data_vars:\n",
    "        if {\"y\", \"x\"}.issubset(ds[var].dims):\n",
    "            existing = ds[var].attrs.get(\"coordinates\", \"\").split()\n",
    "            ds[var].attrs[\"coordinates\"] = \" \".join(\n",
    "                sorted(set(existing) | {\"latitude\", \"longitude\"})\n",
    "            )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def add_latlon(\n",
    "    path: str, ds: xr.Dataset, transformer: pyproj.Transformer\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Wrapper for safe application on a node dataset.\"\"\"\n",
    "    if ds is None:\n",
    "        print(path, \"no dataset\")\n",
    "        return ds\n",
    "    if not {\"x\", \"y\"}.issubset(ds.dims):\n",
    "        print(path, \"not both x,y\")\n",
    "        return ds\n",
    "    return _add_latlon(ds, transformer)\n",
    "\n",
    "\n",
    "def add_latlon_to_dt(dt: xr.DataTree) -> xr.DataTree:\n",
    "    \"\"\"Return a new DataTree with latitude/longitude coords added everywhere possible.\"\"\"\n",
    "    crs_code = dt.attrs[\"other_metadata\"][\"horizontal_CRS_code\"]\n",
    "    src_crs = pyproj.CRS.from_string(crs_code)\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        src_crs, pyproj.CRS.from_epsg(4326), always_xy=True\n",
    "    )\n",
    "    return xr.DataTree.from_dict(\n",
    "        {\n",
    "            path: add_latlon(path, node.ds, transformer)\n",
    "            for path, node in dt.subtree_with_keys\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "latlon_dt = add_latlon_to_dt(small_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_dt[\"measurements\"][\"reflectance\"][\"r10m\"][\"b02\"].plot(\n",
    "    x=\"longitude\", y=\"latitude\"\n",
    ")\n",
    "latlon_dt[\"measurements\"][\"reflectance\"][\"r10m\"][\"b02\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "\n",
    "crs_code = latlon_dt.attrs[\"other_metadata\"][\"horizontal_CRS_code\"]\n",
    "\n",
    "# Option 1: let pyproj parse the whole string\n",
    "src_crs = pyproj.CRS.from_string(crs_code)\n",
    "\n",
    "# Option 2: strip the prefix and give only the integer\n",
    "# src_crs = pyproj.CRS.from_epsg(int(crs_code.split(\":\")[-1]))\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    src_crs, pyproj.CRS.from_epsg(4326), always_xy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "from foscat.Plot import lgnomproject\n",
    "\n",
    "ds = latlon_dt[\"measurements\"][\"reflectance\"][\"r10m\"][\"b02\"]\n",
    "\n",
    "level = 20\n",
    "lon = ds[\"longitude\"].values.ravel()\n",
    "lat = ds[\"latitude\"].values.ravel()\n",
    "\n",
    "# Convert lon/lat to HEALPix spherical angles\n",
    "theta = np.deg2rad(90.0 - lat)\n",
    "phi = np.deg2rad(lon)\n",
    "\n",
    "# Compute interpolation weights and indices for projecting from HEALPix to the UTM grid\n",
    "interpolation_cell_ids, _ = hp.get_interp_weights(2**level, theta, phi, nest=True)\n",
    "\n",
    "# compute square of the distance\n",
    "vecr = hp.ang2vec(theta, phi)\n",
    "xc, yc, zc = hp.pix2vec(2**level, interpolation_cell_ids, nest=True)\n",
    "distance2 = (4**level) * (\n",
    "    (vecr[None, :, 0] - xc) ** 2\n",
    "    + (vecr[None, :, 1] - yc) ** 2\n",
    "    + (vecr[None, :, 2] - zc) ** 2\n",
    ")\n",
    "\n",
    "# compute weights\n",
    "w = np.exp(-distance2)\n",
    "w /= np.sum(w, 0)[None, :]\n",
    "\n",
    "# Identify the unique HEALPix cells that will contribute to u\n",
    "cell_ids, indices = np.unique(interpolation_cell_ids, return_inverse=True)\n",
    "\n",
    "# compute distance\n",
    "imh = np.bincount(indices.flatten(), w.flatten())\n",
    "im = np.bincount(indices.flatten(), w.flatten() * np.tile(ds.values.flatten(), 4))\n",
    "w.shape, ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "lgnomproject(cell_ids, im / imh, 2**level, hold=False, xsize=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 5. **Inverse Problem Solution**  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "26",
   "metadata": {},
   "source": [
    "import foscat.SphericalStencil as sc\n",
    "\n",
    "# Define the convolution width (effective PSF parameter)\n",
    "# Here: σ = √2\n",
    "sigma = 1.07 #np.sqrt(2)\n",
    "\n",
    "def conjugate_gradient_normal_equation(data, x0, www, all_idx, LPT=None, LP=None,\n",
    "                                       max_iter=100, tol=1e-8, verbose=True):\n",
    "    \"\"\"\n",
    "    Solve the normal equation (Pᵗ P) x = Pᵗ y using the Conjugate Gradient method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data    : array_like\n",
    "        Observed UTM data y ∈ ℝᵐ\n",
    "    x0      : array_like\n",
    "        Initial guess for solution x ∈ ℝⁿ (HEALPix domain)\n",
    "    www     : interpolation weights\n",
    "    all_idx : interpolation indices\n",
    "    LPT     : implementation of adjoint operator Pᵗ\n",
    "    LP      : implementation of forward operator P\n",
    "    max_iter: maximum number of CG iterations\n",
    "    tol     : stopping tolerance on residual norm\n",
    "    verbose : print convergence info every 50 iterations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : estimated HEALPix solution u ∈ ℝⁿ\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def default_P(x, W, indices):\n",
    "        \"\"\"\n",
    "        Forward operator: P(x) = projection of HEALPix map x onto the UTM grid.\n",
    "\n",
    "        Steps:\n",
    "        - Apply spherical convolution with kernel w(x,y).\n",
    "        - Interpolate from HEALPix cells to UTM pixels using weights W and indices.\n",
    "        \"\"\"\n",
    "        return np.sum(x[indices] * W, 0)\n",
    "\n",
    "    def default_PT(y, W, indices, hit):\n",
    "        \"\"\"\n",
    "        Adjoint operator: Pᵗ(y) = back-projection from UTM grid to HEALPix cells.\n",
    "    \n",
    "        Steps:\n",
    "        - Distribute UTM values y back onto contributing HEALPix cells using W.\n",
    "        - Apply hit normalization (inverse of pixel coverage).\n",
    "        - Apply spherical convolution with kernel w(x,y).\n",
    "        \"\"\"\n",
    "        value = np.bincount(indices.flatten(),weights=(W * y[None,:]).flatten()) * hit\n",
    "        return value\n",
    "            \n",
    "    if LPT is None:\n",
    "        LPT=default_P\n",
    "        LPT=default_PT\n",
    "        \n",
    "    x = x0.copy()\n",
    "\n",
    "    # Compute pixel coverage normalization (hit map)\n",
    "    hit = np.bincount(all_idx.flatten(), weights=www.flatten())\n",
    "    hit[hit > 0] = 1 / hit[hit > 0]\n",
    "\n",
    "    # Compute b = Pᵗ y\n",
    "    b = LPT(data, www, all_idx, hit)\n",
    "\n",
    "    # Compute initial residual r = b - A x, with A = Pᵗ P\n",
    "    Ax = LPT(LP(x, www, all_idx), www, all_idx, hit)\n",
    "    r = b - Ax\n",
    "\n",
    "    # Initialize direction\n",
    "    p = r.copy()\n",
    "    rs_old = np.dot(r, r)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Compute A p = Pᵗ P p\n",
    "        Ap = LPT(LP(p, www, all_idx), www, all_idx, hit)\n",
    "\n",
    "        alpha = rs_old / np.dot(p, Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "\n",
    "        rs_new = np.dot(r, r)\n",
    "\n",
    "        if verbose and i % 50 == 0:\n",
    "            print(f\"Iter {i:03d}: residual = {np.sqrt(rs_new):.3e}\")\n",
    "\n",
    "        if np.sqrt(rs_new) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged. Iter {i:03d}: residual = {np.sqrt(rs_new):.3e}\")\n",
    "            break\n",
    "\n",
    "        p = r + (rs_new / rs_old) * p\n",
    "        rs_old = rs_new\n",
    "\n",
    "    print(f\"Iter {i:03d}: residual = {np.sqrt(rs_new):.3e}\")\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Conversion to HEALPix\n",
    "\n",
    "We check dimensions and metadata (e.g., `cell_ids`, level, indexing scheme) and ensure variables were preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import healpy as hp\n",
    "\n",
    "# from healpix_geo.nested import lonlat_to_healpix\n",
    "\n",
    "\n",
    "# --- level selection (coarsest grid not finer than dx) ---\n",
    "EARTH_RADIUS_M = 6_371_000.0  # radius used in healpix-geo levels table\n",
    "\n",
    "\n",
    "def _healpix_edge_length_m(level: int, radius_m: float = EARTH_RADIUS_M) -> float:\n",
    "    # edge = R * sqrt(pi/3) / 2**level  (matches healpix-geo \"levels\" page)\n",
    "    return radius_m * np.sqrt(np.pi / 3.0) / (2**level)\n",
    "\n",
    "\n",
    "def _infer_dx_from_x(ds: xr.Dataset) -> float:\n",
    "    x = np.asarray(ds[\"x\"].values)\n",
    "    dx = float(np.nanmedian(np.abs(np.diff(x))))\n",
    "    if not np.isfinite(dx) or dx <= 0:\n",
    "        raise ValueError(\"Could not infer a positive spacing from ds['x'].\")\n",
    "    return dx\n",
    "\n",
    "\n",
    "def _choose_healpix_level_from_dx(\n",
    "    ds: xr.Dataset, min_level: int = 0, max_level: int = 29\n",
    ") -> int:\n",
    "    dx = _infer_dx_from_x(ds)\n",
    "    base = EARTH_RADIUS_M * np.sqrt(np.pi / 3.0)\n",
    "    level = int(np.floor(np.log2(base / dx))) + 1  # edge(level) >= dx\n",
    "    return int(np.clip(level, min_level, max_level))\n",
    "\n",
    "\n",
    "# --- single-dataset transform -> grouped by unique HEALPix cell_ids ---\n",
    "def _to_healpix_cells_grouped_mean(\n",
    "    ds: xr.Dataset,\n",
    "    level: int | None = None,\n",
    "    ellipsoid: str = \"WGS84\",\n",
    "    mask_info: bool = False,\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Returns a dataset with dims (angle, cell_ids), where 'cell_ids' is a\n",
    "    dimension/coordinate containing unique HEALPix ids (NESTED).\n",
    "    Values are averaged over all source samples that mapped to the same cell.\n",
    "    \"\"\"\n",
    "    if not {\"y\", \"x\"}.issubset(ds.dims):\n",
    "        raise ValueError(\"Dataset must have 'y' and 'x' dimensions.\")\n",
    "    if not {\"latitude\", \"longitude\"}.issubset(ds.coords):\n",
    "        raise ValueError(\n",
    "            \"Dataset must have 'latitude' and 'longitude' coords (degrees).\"\n",
    "        )\n",
    "\n",
    "    if level is None:\n",
    "        level = _choose_healpix_level_from_dx(ds)\n",
    "\n",
    "    # 1) hash each (lon,lat) to HEALPix nested cell id\n",
    "    lon = ds[\"longitude\"].values.ravel()\n",
    "    lat = ds[\"latitude\"].values.ravel()\n",
    "\n",
    "    # Convert lon/lat to HEALPix spherical angles\n",
    "    theta = np.deg2rad(90.0 - lat)\n",
    "    phi = np.deg2rad(lon)\n",
    "\n",
    "    nside = 2**level\n",
    "\n",
    "    if mask_info:\n",
    "        # Compute interpolation weights and indices for projecting from HEALPix to the UTM grid\n",
    "        # it is not to use it in the mask computation, but to have the same cell_ids than the other data\n",
    "        interpolation_cell_ids, _ = hp.get_interp_weights(\n",
    "            2**level, theta, phi, nest=True\n",
    "        )\n",
    "\n",
    "        # Identify the unique HEALPix cells that will contribute to u\n",
    "        cell_ids, indices = np.unique(interpolation_cell_ids, return_inverse=True)\n",
    "\n",
    "        l_cell_ids = hp.ang2pix(nside, theta, phi, nest=True)\n",
    "\n",
    "        # Identify the unique HEALPix cells that will contribute to u\n",
    "        indices = np.searchsorted(cell_ids, l_cell_ids)\n",
    "\n",
    "        o_cell_ids = cell_ids.astype(\"int64\")\n",
    "\n",
    "        # now I need to merge flags\n",
    "        N = o_cell_ids.size  # number of HEALPix cells\n",
    "\n",
    "        cell_dim = \"cells\"  # name of the new dimension\n",
    "        cid_name = \"cell_ids\"  # name of the coordinate storing HEALPix IDs\n",
    "\n",
    "        # 1) Build the 1D Dataset\n",
    "        data_vars = {}\n",
    "        for v in ds.data_vars:\n",
    "            patch = ds[v].values\n",
    "            # run your solver to compute values instead of zeros\n",
    "            vals = np.zeros([N], dtype=\"uint8\")\n",
    "\n",
    "            for k in range(8):\n",
    "                vals += (\n",
    "                    np.bincount(\n",
    "                        indices.flatten(), (patch.flatten() >> k) & 1, minlength=N\n",
    "                    )\n",
    "                    > 0\n",
    "                ).astype(\"uint8\") * 2**k\n",
    "\n",
    "            # Each variable must have shape (cell,)\n",
    "            data_vars[v] = ((cell_dim,), vals)\n",
    "\n",
    "            # 2) Create the Dataset with the HEALPix cell_ids as a coordinate\n",
    "            out = xr.Dataset(\n",
    "                data_vars=data_vars, coords={cid_name: ((cell_dim,), o_cell_ids)}\n",
    "            )\n",
    "\n",
    "        # 4) Add metadata to the cell_ids coordinate\n",
    "        out[cid_name].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "\n",
    "        # 3) Use the cell_ids coordinate as the index of the 'cell' dimension\n",
    "        out = out.set_index({cell_dim: cid_name})\n",
    "\n",
    "        # 5) Copy global attributes from the original dataset\n",
    "        out.attrs = dict(ds.attrs)\n",
    "        # 6) Copy variable-specific attributes from the original dataset\n",
    "        for v in ds.data_vars:\n",
    "            out[v].attrs = dict(ds[v].attrs)\n",
    "\n",
    "    elif lon.shape[0] < 100:  # do not convert with PSF deconv small data\n",
    "        o_cell_ids = hp.ang2pix(nside, theta, phi, nest=True)\n",
    "\n",
    "        # cell_ids = lonlat_to_healpix(lon, lat, level, ellipsoid=ellipsoid)\n",
    "        # 2) stack (y,x) -> cells\n",
    "        out = ds.stack(cells=(\"y\", \"x\"))\n",
    "\n",
    "        # 3) attach cell_ids coord on 'cells'\n",
    "        out = out.assign_coords(cell_ids=(\"cells\", o_cell_ids.astype(\"int64\")))\n",
    "        out[\"cell_ids\"].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "        cell_ids_attrs = dict(out[\"cell_ids\"].attrs)  # keep for after groupby\n",
    "\n",
    "        # 4) drop redundant coords/vars\n",
    "        #    drop_these = [n for n in (\"x\", \"y\", \"latitude\", \"longitude\") if n in out.variables]\n",
    "        #    out = out.drop_vars(drop_these)\n",
    "\n",
    "        # 5) group by cell_ids and average\n",
    "        # **note** This is a very simplified test conversion,\n",
    "        # later this should be updated spline other interpolation methods.\n",
    "        out = out.groupby(\"cell_ids\").mean().rename_dims(cell_ids=\"cells\")\n",
    "        # 6) restore attrs on the new dimension coordinate\n",
    "        if \"cell_ids\" in out.coords:\n",
    "            out[\"cell_ids\"].attrs.update(cell_ids_attrs)\n",
    "    else:\n",
    "        # Compute interpolation weights and indices for projecting from HEALPix to the UTM grid\n",
    "        interpolation_cell_ids, _ = hp.get_interp_weights(\n",
    "            2**level, theta, phi, nest=True\n",
    "        )\n",
    "\n",
    "        # Identify the unique HEALPix cells that will contribute to u\n",
    "        cell_ids, indices = np.unique(interpolation_cell_ids, return_inverse=True)\n",
    "\n",
    "        # compute square of the distance\n",
    "        vecr = hp.ang2vec(theta, phi)\n",
    "        xc, yc, zc = hp.pix2vec(2**level, interpolation_cell_ids, nest=True)\n",
    "        distance2 = (4**level) * (\n",
    "            (vecr[None, :, 0] - xc) ** 2\n",
    "            + (vecr[None, :, 1] - yc) ** 2\n",
    "            + (vecr[None, :, 2] - zc) ** 2\n",
    "        )\n",
    "\n",
    "        # compute weights\n",
    "        w = np.exp(-distance2)\n",
    "        w /= np.sum(w, 0)[None, :]\n",
    "\n",
    "        o_cell_ids = cell_ids.astype(\"int64\")\n",
    "\n",
    "        N = o_cell_ids.size  # number of HEALPix cells\n",
    "\n",
    "        cell_dim = \"cells\"  # name of the new dimension\n",
    "        cid_name = \"cell_ids\"  # name of the coordinate storing HEALPix IDs\n",
    "\n",
    "        # compute hit count map\n",
    "        hit_var = np.bincount(indices.flatten(), w.flatten())\n",
    "\n",
    "        # 1) Build the 1D Dataset\n",
    "        test_rgb = False\n",
    "\n",
    "        data_vars = {}\n",
    "        data_vars[\"hits\"] = ((cell_dim,), hit_var)\n",
    "        for v in ds.data_vars:\n",
    "            patch = ds[v].values\n",
    "            if len(patch.shape) > 2:  # RGB\n",
    "                test_rgb = True\n",
    "\n",
    "                vals = []\n",
    "                for k in range(3):\n",
    "                    vals.append(\n",
    "                        np.bincount(\n",
    "                            indices.flatten(),\n",
    "                            w.flatten()\n",
    "                            * np.tile(patch[k].flatten().astype(\"float\"), 4),\n",
    "                        )\n",
    "                        / hit_var\n",
    "                    )\n",
    "                vals = np.vstack(np.clip(vals, 0, 255)).astype(\"uint8\")\n",
    "                data_vars[v] = (\n",
    "                    (\n",
    "                        \"band\",\n",
    "                        cell_dim,\n",
    "                    ),\n",
    "                    vals,\n",
    "                )\n",
    "            else:\n",
    "                # run your solver to compute values instead of zeros\n",
    "                vals = (\n",
    "                    np.bincount(\n",
    "                        indices.flatten(),\n",
    "                        w.flatten() * np.tile(patch.flatten().astype(\"float\"), 4),\n",
    "                    )\n",
    "                    / hit_var\n",
    "                )\n",
    "\n",
    "                # Each variable must have shape (cell,)\n",
    "                data_vars[v] = ((cell_dim,), vals)\n",
    "        if test_rgb:\n",
    "            # 2) Create the Dataset with the HEALPix cell_ids as a coordinate\n",
    "            out = xr.Dataset(\n",
    "                data_vars=data_vars,\n",
    "                coords={\"band\": np.arange(3), cid_name: ((cell_dim,), o_cell_ids)},\n",
    "            )\n",
    "        else:\n",
    "            # 2) Create the Dataset with the HEALPix cell_ids as a coordinate\n",
    "            out = xr.Dataset(\n",
    "                data_vars=data_vars, coords={cid_name: ((cell_dim,), o_cell_ids)}\n",
    "            )\n",
    "\n",
    "        # 4) Add metadata to the cell_ids coordinate\n",
    "        out[cid_name].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "\n",
    "        # 3) Use the cell_ids coordinate as the index of the 'cell' dimension\n",
    "        out = out.set_index({cell_dim: cid_name})\n",
    "\n",
    "        # 5) Copy global attributes from the original dataset\n",
    "        out.attrs = dict(ds.attrs)\n",
    "        # 6) Copy variable-specific attributes from the original dataset\n",
    "        for v in ds.data_vars:\n",
    "            out[v].attrs = dict(ds[v].attrs)\n",
    "\n",
    "    # 7) keep order stable for variables like (angle, cell_ids)\n",
    "    #   for v in out.data_vars:\n",
    "    #       if (\"angle\" in out[v].dims) and (\"cell_ids\" in out[v].dims):\n",
    "    #           out[v] = out[v].transpose(\"angle\", \"cell_ids\", ...)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _update_group_metadata(\n",
    "    ds: xr.Dataset,\n",
    "    level: int,\n",
    "    ellipsoid: str = \"WGS84\",\n",
    "    grid_mapping_var_name: str = \"crs\",\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Update metadata of a single group.\"\"\"\n",
    "\n",
    "    if \"cells\" not in ds.dims:\n",
    "        return ds\n",
    "\n",
    "    ds_healpix = ds.copy()\n",
    "\n",
    "    # create healpix grid mapping variable\n",
    "    ds_healpix.coords[grid_mapping_var_name] = (\n",
    "        (),\n",
    "        0,\n",
    "        {\n",
    "            \"grid_mapping_name\": \"healpix\",\n",
    "            \"indexing_scheme\": \"nested\",\n",
    "            \"refinement_ratio\": 4,\n",
    "            \"refinement_level\": level,\n",
    "            \"reference_body\": ellipsoid,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # update variable attributes\n",
    "    # dimensions, coordinates and grid_mapping attributes\n",
    "    # clean-up non-healpix variable attributes\n",
    "    for var_name, var in ds_healpix.variables.items():\n",
    "        if \"cells\" in var.dims:\n",
    "            attrs = var.attrs.copy()\n",
    "            eopf_attrs = attrs.get(\"_eopf_attrs\", None)\n",
    "            if eopf_attrs is not None:\n",
    "                eopf_attrs[\"dimensions\"] = [\"cells\"]\n",
    "                eopf_attrs[\"coordinates\"] = [\"cell_ids\"]\n",
    "            for name in var.attrs:\n",
    "                if \"proj:\" in name:\n",
    "                    del attrs[name]\n",
    "            if \"coordinates\" in var.attrs:\n",
    "                attrs[\"coordinates\"] = \"cell_ids\"\n",
    "            if var_name != \"cell_ids\":\n",
    "                attrs[\"grid_mapping\"] = grid_mapping_var_name\n",
    "            var.attrs = attrs\n",
    "\n",
    "    return ds_healpix\n",
    "\n",
    "\n",
    "# --- HEALPix group converter ---\n",
    "def _add_healpix_to_group(path: str, ds: xr.Dataset) -> tuple[xr.Dataset, int | None]:\n",
    "    if ds is None:\n",
    "        print(path, \"no dataset — keeping empty group\")\n",
    "        return xr.Dataset()\n",
    "\n",
    "    has_xy = {\"x\", \"y\"}.issubset(ds.dims)\n",
    "    has_ll = {\"latitude\", \"longitude\"}.issubset(ds.coords)\n",
    "\n",
    "    if has_xy and not has_ll:\n",
    "        # stop the whole operation as requested\n",
    "        raise RuntimeError(\n",
    "            f\"{path}: has x/y but missing latitude/longitude — aborting.\"\n",
    "        )\n",
    "\n",
    "    if has_ll and has_xy:\n",
    "        level = _choose_healpix_level_from_dx(ds)\n",
    "        print(\n",
    "            f\"{path}: chosen level {level} (edge≈{_healpix_edge_length_m(level):.4f} m)\"\n",
    "        )\n",
    "        ds_healpix = _to_healpix_cells_grouped_mean(\n",
    "            ds, level=level, ellipsoid=\"WGS84\", mask_info=\"mask\" in path\n",
    "        )\n",
    "        ds_healpix = _update_group_metadata(ds_healpix, level=level, ellipsoid=\"WGS84\")\n",
    "        return ds_healpix, level\n",
    "\n",
    "    # no lat/lon -> do nothing\n",
    "    print(path, \"no latitude/longitude — skipping\")\n",
    "    return ds, None\n",
    "\n",
    "\n",
    "def _update_stac_discovery(attrs: dict):\n",
    "    \"\"\"Update product level STAC.\"\"\"\n",
    "\n",
    "    # TODO: update \"bbox\" and \"geometry\" attributes to\n",
    "    # so it is consistent with HEALPix cell geometries\n",
    "\n",
    "    props = attrs[\"properties\"].copy()\n",
    "\n",
    "    # remove irrelevant properties for HEALPix\n",
    "    for k in attrs[\"properties\"]:\n",
    "        if \"proj:\" in k:\n",
    "            del props[k]\n",
    "\n",
    "    # TODO: add STAC discovery \"grid:code\" property\n",
    "    # \"grid:code\" value is formatted like \"HEALPIX-I{indexing_scheme}-L{level}-{cell_id}\"\n",
    "    # use a placeholder cell id value for now (to be defined)\n",
    "    # props[\"grid:code\"] = \"HEALPIX-Inested-L10-1234\"\n",
    "    # attrs[\"stac_extensions\"].append(\n",
    "    #    \"https://stac-extensions.github.io/grid/v1.1.0/schema.json\"\n",
    "    # )\n",
    "\n",
    "    # TODO: update \"bbox\" and \"geometry\" attribute values.\n",
    "\n",
    "    attrs[\"properties\"] = props\n",
    "\n",
    "\n",
    "# --- public function: apply over the whole DataTree ---\n",
    "def add_healpix_to_dt(dt: xr.DataTree) -> xr.DataTree:\n",
    "    \"\"\"Transform nodes to HEALPix (grouped mean per cell) where possible; preserve others.\"\"\"\n",
    "\n",
    "    # HEALPix multiscale attribute convention\n",
    "    # (inspired from https://github.com/zarr-developers/geozarr-spec/issues/83#issuecomment-3292459330)\n",
    "    multiscales_attr = {\n",
    "        \"name\": \"healpix\",\n",
    "        \"configuration\": {\"refinement_ratio\": 4},\n",
    "    }\n",
    "    multiscale_paths = set()\n",
    "\n",
    "    # process datatree group by group\n",
    "    updated_groups = {}\n",
    "    for path, group in dt.subtree_with_keys:\n",
    "        path = pathlib.Path(path)\n",
    "        ds_healpix, level = _add_healpix_to_group(str(path), group.ds)\n",
    "\n",
    "        # detect multiscale and rename group according to\n",
    "        # HEALPix refinement level\n",
    "        if level is not None:\n",
    "            path = path.parent / str(level)\n",
    "            multiscale_paths.add(str(path.parent))\n",
    "\n",
    "        updated_groups[str(path)] = ds_healpix\n",
    "\n",
    "    dt_healpix = xr.DataTree.from_dict(updated_groups, name=getattr(dt, \"name\", None))\n",
    "\n",
    "    # add multiscale metadata\n",
    "    for path, group in dt_healpix.subtree_with_keys:\n",
    "        if path in multiscale_paths:\n",
    "            group.attrs[\"multiscales\"] = multiscales_attr\n",
    "\n",
    "    # update product level metadata\n",
    "    _update_stac_discovery(dt_healpix.attrs[\"stac_discovery\"])\n",
    "\n",
    "    return dt_healpix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# compute ONE patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_pieces(dt, x_beg, x_end, y_beg, y_end):\n",
    "    small_dt = dt.sel(\n",
    "        x=slice(x_beg, x_end),\n",
    "        y=slice(y_beg, y_end),\n",
    "    )\n",
    "    latlon_dt = add_latlon_to_dt(small_dt)\n",
    "\n",
    "    # Convert the whole tree\n",
    "    return add_healpix_to_dt(latlon_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# Utilities for data tree merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def has_healpix_index(ds: xr.Dataset, cell_dim=\"cells\") -> bool:\n",
    "    return (cell_dim in ds.dims) or (\"cell_ids\" in ds.dims) or (\"cell_ids\" in ds.coords)\n",
    "\n",
    "\n",
    "def ensure_cell_dim_preserve_ids(ds: xr.Dataset, *, cell_dim=\"cells\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Ensure the dataset has a single HEALPix dimension `cell_dim` whose\n",
    "    *dimension coordinate values are the true HEALPix IDs* (int64).\n",
    "    Rules:\n",
    "      - If dim is already `cell_dim` and a coord `cell_ids` exists with same length,\n",
    "        use its values for the dim coordinate (then drop the extra coord).\n",
    "      - If the dim is named 'cell_ids', rename dim AND coordinate to `cell_dim`,\n",
    "        keeping values (the true IDs).\n",
    "      - If only a non-dim coord 'cell_ids' exists, promote it as the dim coordinate.\n",
    "    \"\"\"\n",
    "    ds2 = ds\n",
    "\n",
    "    # Case 1: target dim already present\n",
    "    if cell_dim in ds2.dims:\n",
    "        # If there is a coord 'cell_ids' aligned on this dim and it differs, use it.\n",
    "        if \"cell_ids\" in ds2.coords and ds2[\"cell_ids\"].dims == (cell_dim,):\n",
    "            cid = xr.DataArray(ds2[\"cell_ids\"]).astype(\"int64\")\n",
    "            if cid.sizes[cell_dim] == ds2.sizes[cell_dim]:\n",
    "                # Replace the dimension coordinate values by the true IDs\n",
    "                ds2 = ds2.assign_coords({cell_dim: cid})\n",
    "                # Remove the extra coord to avoid future alignment conflicts\n",
    "                ds2 = ds2.drop_vars(\"cell_ids\")\n",
    "        # Ensure the dim coord exists and is int64\n",
    "        if (cell_dim not in ds2.coords) or (ds2[cell_dim].dims != (cell_dim,)):\n",
    "            ds2 = ds2.assign_coords({cell_dim: (cell_dim, ds2[cell_dim].values)})\n",
    "        ds2 = ds2.assign_coords({cell_dim: ds2[cell_dim].astype(\"int64\")})\n",
    "        ds2[cell_dim].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "        return ds2\n",
    "\n",
    "    # Case 2: the dimension is named 'cell_ids' -> rename dim *and* coord to `cell_dim`\n",
    "    if \"cell_ids\" in ds2.dims:\n",
    "        ren_dims = {\"cell_ids\": cell_dim}\n",
    "        ren_vars = {\"cell_ids\": cell_dim} if \"cell_ids\" in ds2.coords else {}\n",
    "        ds2 = ds2.rename_dims(ren_dims).rename_vars(ren_vars)\n",
    "        # Ensure dim coord exists & int64\n",
    "        if (cell_dim not in ds2.coords) or (ds2[cell_dim].dims != (cell_dim,)):\n",
    "            ds2 = ds2.assign_coords({cell_dim: (cell_dim, ds2[cell_dim].values)})\n",
    "        ds2 = ds2.assign_coords({cell_dim: ds2[cell_dim].astype(\"int64\")})\n",
    "        ds2[cell_dim].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "        return ds2\n",
    "\n",
    "    # Case 3: only a non-dim coord 'cell_ids' exists -> promote it as dim coord\n",
    "    if \"cell_ids\" in ds2.coords:\n",
    "        cid = np.asarray(ds2[\"cell_ids\"].values).ravel().astype(\"int64\")\n",
    "        ds2 = xr.Dataset(\n",
    "            data_vars={k: v for k, v in ds2.data_vars.items()},\n",
    "            coords={\n",
    "                cell_dim: (cell_dim, cid),\n",
    "                **{k: v for k, v in ds2.coords.items() if k != \"cell_ids\"},\n",
    "            },\n",
    "        )\n",
    "        ds2[cell_dim].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "        return ds2\n",
    "\n",
    "    raise ValueError(\n",
    "        \"No HEALPix index found: need 'cells' dim, 'cell_ids' dim, or 'cell_ids' coord.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_two_healpix_union_mean_or(\n",
    "    ds1: xr.Dataset, ds2: xr.Dataset, *, cell_dim=\"cells\"\n",
    ") -> xr.Dataset:\n",
    "    ds1 = ensure_cell_dim_preserve_ids(ds1, cell_dim=cell_dim)\n",
    "    ds2 = ensure_cell_dim_preserve_ids(ds2, cell_dim=cell_dim)\n",
    "\n",
    "    all_cells = np.union1d(ds1[cell_dim].values, ds2[cell_dim].values)\n",
    "\n",
    "    def _reindex(ds):\n",
    "        ds_r = ds.reindex({cell_dim: all_cells}, fill_value=None)\n",
    "        fill_vals = {\n",
    "            name: (0 if ds[name].dtype == np.uint8 else np.nan)\n",
    "            for name in ds_r.data_vars\n",
    "        }\n",
    "        return ds_r.fillna(fill_vals)\n",
    "\n",
    "    ds1r, ds2r = _reindex(ds1), _reindex(ds2)\n",
    "\n",
    "    merged_vars = {}\n",
    "    for name in ds1r.data_vars:\n",
    "        a, b = xr.align(ds1r[name], ds2r[name], join=\"outer\")\n",
    "        if a.dtype == np.uint8:\n",
    "            merged = xr.apply_ufunc(\n",
    "                np.bitwise_or,\n",
    "                a.astype(np.uint8),\n",
    "                b.astype(np.uint8),\n",
    "                dask=\"parallelized\",\n",
    "                output_dtypes=[np.uint8],\n",
    "            )\n",
    "        else:\n",
    "            merged = xr.concat([a, b], dim=\"_patch\").mean(\"_patch\", skipna=True)\n",
    "        merged_vars[name] = merged\n",
    "\n",
    "    out = xr.Dataset(\n",
    "        merged_vars, coords={cell_dim: (cell_dim, all_cells.astype(\"int64\"))}\n",
    "    )\n",
    "    out.attrs = dict(ds1.attrs)\n",
    "    for v in out.data_vars:\n",
    "        if v in ds1:\n",
    "            out[v].attrs = dict(ds1[v].attrs)\n",
    "    out[cell_dim].attrs.update({\"standard_name\": \"healpix\", \"units\": \"1\"})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# Merge a list of Datasets (one group) by mean / bitwise-OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "def _groups_from_patch(\n",
    "    patch, only_healpix: bool, cell_dim=\"cells\"\n",
    ") -> dict[str, xr.Dataset]:\n",
    "    \"\"\"Supporte xr.DataTree, dict[str, xr.Dataset], xr.Dataset.\"\"\"\n",
    "    out = {}\n",
    "    if hasattr(patch, \"subtree_with_keys\"):  # xr.DataTree-like\n",
    "        for path, node in patch.subtree_with_keys:\n",
    "            ds = getattr(node, \"ds\", None)\n",
    "            if isinstance(ds, xr.Dataset) and len(ds.data_vars) > 0:\n",
    "                if not only_healpix or has_healpix_index(ds, cell_dim=cell_dim):\n",
    "                    out[str(path)] = ds\n",
    "        return out\n",
    "    if isinstance(patch, dict):  # dict[path] = Dataset\n",
    "        for path, ds in patch.items():\n",
    "            if isinstance(ds, xr.Dataset) and len(ds.data_vars) > 0:\n",
    "                if not only_healpix or has_healpix_index(ds, cell_dim=cell_dim):\n",
    "                    out[str(path)] = ds\n",
    "        return out\n",
    "    if isinstance(patch, xr.Dataset):  # single dataset at root\n",
    "        if len(patch.data_vars) > 0 and (\n",
    "            not only_healpix or has_healpix_index(patch, cell_dim=cell_dim)\n",
    "        ):\n",
    "            out[\"/\"] = patch\n",
    "        return out\n",
    "    raise TypeError(\"Unsupported patch type.\")\n",
    "\n",
    "\n",
    "def merge_patches_groupwise_into_initial_dt(\n",
    "    initial_dt: xr.DataTree,\n",
    "    patches: list,  # éléments: xr.DataTree | dict[str, xr.Dataset] | xr.Dataset\n",
    "    *,\n",
    "    cell_dim=\"cells\",\n",
    "    only_healpix=True,\n",
    "    keep_non_healpix: str | None = \"first\",\n",
    "    multiscales_attr: dict | None = None,\n",
    ") -> xr.DataTree:\n",
    "    \"\"\"\n",
    "    Fusionne tous les `patches` dans `initial_dt` et retourne un xr.DataTree.\n",
    "    - HEALPix: union + mean/OR\n",
    "    - non-HEALPix: gardé tel quel (ou ignoré si keep_non_healpix=None)\n",
    "    - Attributs: priorité à ceux de `initial_dt`\n",
    "    - Optionnel: ajoute un attr 'multiscales' sur des chemins parent (si tu veux)\n",
    "    \"\"\"\n",
    "    # 1) collecter les groupes de l'initial et des patches\n",
    "    init_map = _groups_from_patch(initial_dt, only_healpix=False, cell_dim=cell_dim)\n",
    "    patch_maps = [\n",
    "        _groups_from_patch(p, only_healpix=only_healpix, cell_dim=cell_dim)\n",
    "        for p in patches\n",
    "    ]\n",
    "\n",
    "    paths = set(init_map.keys())\n",
    "    for m in patch_maps:\n",
    "        paths |= set(m.keys())\n",
    "\n",
    "    updated_groups: dict[str, xr.Dataset] = {}\n",
    "    multiscale_paths = set()\n",
    "\n",
    "    # 2) fusion par chemin\n",
    "    for path in sorted(paths):\n",
    "        base = init_map.get(path)\n",
    "        others = [m[path] for m in patch_maps if path in m]\n",
    "\n",
    "        # déterminer si groupe HEALPix\n",
    "        probe = base if base is not None else (others[0] if others else None)\n",
    "        if probe is None:\n",
    "            continue\n",
    "        is_hp = has_healpix_index(probe, cell_dim=cell_dim)\n",
    "\n",
    "        if is_hp:\n",
    "            acc = base if base is not None else others[0]\n",
    "            # start_idx = 0 if base is None else 0\n",
    "            for ds in (others if base is not None else others[1:]):\n",
    "                acc = merge_two_healpix_union_mean_or(acc, ds, cell_dim=cell_dim)\n",
    "\n",
    "            # attrs depuis l'initial si dispo\n",
    "            if base is not None:\n",
    "                acc.attrs = dict(base.attrs)\n",
    "                for v in acc.data_vars:\n",
    "                    if v in base:\n",
    "                        acc[v].attrs = dict(base[v].attrs)\n",
    "\n",
    "            updated_groups[path] = acc\n",
    "\n",
    "            # (optionnel) log multiscale si ton code renomme par niveau\n",
    "            # à adapter si tu veux dériver 'level' ici\n",
    "            # if some_level is not None:\n",
    "            #     parent = str(Path(path).parent)\n",
    "            #     multiscale_paths.add(parent)\n",
    "\n",
    "        else:\n",
    "            # non-HEALPix\n",
    "            if base is not None:\n",
    "                updated_groups[path] = base\n",
    "            elif keep_non_healpix == \"first\" and others:\n",
    "                updated_groups[path] = others[0]\n",
    "            # sinon: ignorer\n",
    "\n",
    "    # 3) reconstruire un xr.DataTree\n",
    "    name = getattr(initial_dt, \"name\", None)\n",
    "    dt_out = xr.DataTree.from_dict(updated_groups, name=name)\n",
    "\n",
    "    # 4) recopier les attrs globaux de l'initial\n",
    "    try:\n",
    "        dt_out.attrs.update(dict(initial_dt.attrs))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 5) (optionnel) ajouter des métadonnées multiscales sur certains chemins\n",
    "    if multiscales_attr and multiscale_paths:\n",
    "        for path, node in dt_out.subtree_with_keys:\n",
    "            if path in multiscale_paths:\n",
    "                node.attrs[\"multiscales\"] = multiscales_attr\n",
    "\n",
    "    return dt_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# pre-loading data may speed-up the conversion depending on network conditions\n",
    "# latlon_dt.load()\n",
    "# Convert the whole tree\n",
    "all_dt = None\n",
    "for k in range(3):  # dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x.shape[0]-1):\n",
    "    for l_ in range(3):  # dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y.shape[0]-1):\n",
    "        print(\"======================================\")\n",
    "        print(f\" Patch {k,l_} start ...\")\n",
    "        local_dt = get_sub_pieces(\n",
    "            dt,\n",
    "            dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x[k],\n",
    "            dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x[k + 1],\n",
    "            dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y[l_],\n",
    "            dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y[l_ + 1],\n",
    "        )\n",
    "\n",
    "        if all_dt is None:\n",
    "            all_dt = local_dt\n",
    "        else:\n",
    "            all_dt = merge_patches_groupwise_into_initial_dt(\n",
    "                initial_dt=all_dt,  # ton xr.DataTree de base\n",
    "                patches=[\n",
    "                    local_dt\n",
    "                ],  # un ou plusieurs patches (xr.DataTree, dict ou Dataset)\n",
    "                cell_dim=\"cells\",\n",
    "                only_healpix=False,\n",
    "                keep_non_healpix=\"first\",\n",
    "                multiscales_attr={\n",
    "                    \"name\": \"healpix\",\n",
    "                    \"configuration\": {\"refinement_ratio\": 4},\n",
    "                },\n",
    "            )\n",
    "        print(f\" Patch {k,l_} computed\")\n",
    "        print(\"======================================\")\n",
    "\n",
    "\n",
    "all_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Save & Export\n",
    "\n",
    "We persist the HEALPix‑indexed data as **Zarr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from foscat.Plot import lgnomproject\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "lgnomproject(\n",
    "    all_dt[\"measurements\"][\"reflectance\"][\"20\"].cells,\n",
    "    all_dt[\"measurements\"][\"reflectance\"][\"20\"][\"b02\"].values,\n",
    "    2**20,\n",
    "    hold=False,\n",
    "    sub=(2, 2, 1),\n",
    "    title=\"measurements/reflectance/20/b02\",\n",
    ")\n",
    "lgnomproject(\n",
    "    all_dt[\"measurements\"][\"reflectance\"][\"19\"].cells,\n",
    "    all_dt[\"measurements\"][\"reflectance\"][\"19\"][\"b05\"].values,\n",
    "    2**19,\n",
    "    hold=False,\n",
    "    sub=(2, 2, 2),\n",
    "    title=\"measurements/reflectance/19/b05\",\n",
    ")\n",
    "lgnomproject(\n",
    "    all_dt[\"measurements\"][\"reflectance\"][\"17\"].cells,\n",
    "    all_dt[\"measurements\"][\"reflectance\"][\"17\"][\"b01\"].values,\n",
    "    2**17,\n",
    "    hold=False,\n",
    "    sub=(2, 2, 3),\n",
    "    title=\"measurements/reflectance/17/b01\",\n",
    ")\n",
    "lgnomproject(\n",
    "    all_dt[\"quality\"][\"l1c_quicklook\"][\"20\"].cells,\n",
    "    all_dt[\"quality\"][\"l1c_quicklook\"][\"20\"][\"tci\"].values.T / 256.0,\n",
    "    2**20,\n",
    "    hold=False,\n",
    "    sub=(2, 2, 4),\n",
    "    title=\"quality/l1c_quicklook/20/tci\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo rechunk here (may be re-use rechunk function justus will propose for climate dt for optimal chunking of healpix?)\n",
    "\n",
    "all_dt.to_zarr(collection + \"_healpix.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Appendix / References\n",
    "\n",
    "- EOPF Sample Notebooks (Sentinel‑2): examples of structure and style for cloud‑native EO workflows.\n",
    "- HEALPix: Górski et al., 2005. *ApJ* 622, 759–771.\n",
    "- PyProj/PROJ: Coordinate transforms between projected and geographic systems.\n",
    "- Xarray DataTree: Hierarchical datasets for multi‑group EO products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "pixi-kernel": {
   "environment": "eopf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
